%Taylor Helms%Qlearning algorithm implementation in MATLAB%This program creates an n*n grid and trains the%agent to find the quickest route from the starting%point of the grid to the end point by exploring the%grid using an option table and updating the reward table.prompt1 = 'Enter n (The dimensions of the n*n grid): ';n = input(prompt1);prompt2 = 'Enter the start state(single integer between 1 and n*n): ';start_state = input(prompt2);prompt4 = 'Enter the final state|goal(Single integer between 1 and n*n): ';final_state = input(prompt4);%Builds the grid with user input and the option table%for the agent to use to explore the grid.N = n * n;T = [N,4];temp = 0;for A = 1:n    for B = 1:n        temp = temp + 1;        if (A ~= 1) && (A ~= n) && (B ~= 1) && (B ~= n)            T(temp,1) = temp - n;            T(temp,2) = temp + n;            T(temp,3) = temp - 1;            T(temp,4) = temp + 1;        end        if (A == 1) && (B == 1)            T(temp,1) = 0;            T(temp,2) = temp + n;            T(temp,3) = 0;            T(temp,4) = temp + 1;        end        if (A == 1) && (B == n)            T(temp,1) = 0;            T(temp,2) = temp + n;            T(temp,3) = temp - 1;            T(temp,4) = 0;        end        if (A == n) && (B == 1)            T(temp,1) = temp - n;            T(temp,2) = 0;            T(temp,3) = 0;            T(temp,4) = temp + 1;        end        if (A == n) && (B == n)            T(temp,1) = temp - n;            T(temp,2) = 0;            T(temp,3) = temp - 1;            T(temp,4) = 0;        end        if (A == 1) && (B ~= 1) && (B ~= n)            T(temp,1) = 0;            T(temp,2) = temp + n;            T(temp,3) = temp - 1;            T(temp,4) = temp + 1;        end        if (A == n) && (B ~= 1) && (B ~= n)            T(temp,1) = temp - n;            T(temp,2) = 0;            T(temp,3) = temp - 1;            T(temp,4) = temp + 1;        end        if (B == 1) && (A ~= 1) && (A ~= n)            T(temp,1) = temp - n;            T(temp,2) = temp + n;            T(temp,3) = 0;            T(temp,4) = temp + 1;        end        if (B == n) && (A ~= 1) && (A ~= n)            T(temp,1) = temp - n;            T(temp,2) = temp + n;            T(temp,3) = temp - 1;            T(temp,4) = 0;        end    endend%Q matrixQ = zeros(N,4);%discount rate : gamma gamma = 0.8;states = [n,n];R = [N,1];  %reward matrixtemp = 0;%fills reward matrix with -1, costing 1 per movefor A=1:n    for B = 1:n        temp = temp + 1;        R(temp) = -1;    endendR(final_state) = 10;  %reward for reaching end is 10for i=1:20  %number of times agent will run through the grid  current = start_state; %current state  while current ~= final_state    %pick a random action, and end up in a random valid state.    possibilities = T(current,:);    [action, next_state] = random_choice(possibilities);    % immediate reward:    imm_r = R(next_state);        %this block is to calculate gamma*max(Q'(s',a'))    future_rewards = [];    future_states = T(next_state,:);    for a = 1:length(future_states)      expected_future_state = future_states(a);      if expected_future_state ~= 0        future_rewards = [future_rewards, Q(next_state, a)];      end    end    discounted_max_future_reward = gamma*max(future_rewards);    %end of block        %update Q value:       q_value = imm_r + discounted_max_future_reward;    Q(current,action) = q_value;        current = next_state;      end  display(Q);  pause(1);  end%this block maps out the quickest path from start to finishshortest_path = [start_state];current_state = start_state;previous_state = start_state;while current_state ~= final_state    best_option = 1;    for a=1:4        if Q(current_state,a) == Q(current_state, best_option)           j = randi(2);          if j == 1              best_option = a;          end        end       if Q(current_state,a) > Q(current_state,best_option)           best_option = a;       end    end    while Q(current_state,best_option) == 0 || T(current_state, best_option) == previous_state        best_option = randi(4);    end    previous_state = current_state;    current_state = T(current_state, best_option);    shortest_path = [shortest_path, current_state];enddisplay(shortest_path);%end of block